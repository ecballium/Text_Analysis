test_sum <- as.data.frame(test_sum)
#test_sum <- test_sum %>% mutate(H_Rank = dense_rank(desc(H)))
test_sum <- test_sum %>% mutate(H_Rank = row_number(desc(H)))
test_sum <- test_sum %>% mutate(Tot_Rank = SAT_Rank+ H_Rank)
test_sum <- test_sum %>% arrange(Tot_Rank)
#' A note on ordering:
#'
#' - decreasing entropy (stopwords have highest entropy)
#' - decreasing SAT_Rank (stopwords have high MP and low VP)
#' NOTE: in case of the same SAT values, MP is taken into account
#'
#'
#'
#'
#'
#' ## Visualisation
#'
#+
MP_1q <- summary(test_sum$MP)[2]
MP_3q <- summary(test_sum$MP)[5]
VP_1q <- summary(test_sum$VP)[2]
VP_3q <- summary(test_sum$VP)[5]
ggplot(test_sum, aes(x = MP, y = VP)) + geom_point(aes(fill = H))
ggplot(test_sum, aes(x = MP, y = VP)) + geom_point(aes(col = H)) + ylim(c(VP_1q, VP_3q)) + xlim(c(MP_1q, MP_3q))+ scale_colour_gradientn(colors = terrain.colors(100))
ggplot(test_sum, aes(x = MP, y = VP)) + geom_point(aes(col = Tot_Rank)) + ylim(c(VP_1q, VP_3q)) + xlim(c(MP_1q, MP_3q)) + scale_colour_gradientn(colors = heat.colors(5))
#heat.colors is a palette from grDevices
ggplot(test_sum, aes(x = SAT_Rank, y = H_Rank)) + geom_point(size = 0.02)
ggplot(test_sum, aes(x = SAT_Rank, y = H_Rank)) + geom_point(size = 0.02, aes(col = MP)) + scale_color_gradient(low = "green", high = "red")
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(0, 0.001))
ggplot(test_sum, aes(x = SAT_Rank, y = H_Rank)) + geom_point(size = 0.02, aes(col = MP)) + sc
View(test_sum)
rmarkdown::render("Scripts/Stoplist_Analysis_Full.R")
rmarkdown::render("Scripts/Stoplist_Analysis_Full.R", encoding = "UTF-8")
encoding(counts_df$words)
Encoding(counts_df$words)
rmarkdown::render("Scripts/Stoplist_Analysis_Full.R", encoding = "UTF-8")
rmarkdown::render("Scripts/Stoplist_Analysis_Full.R", encoding = "UTF-8")
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
qplot(c(1:10), c(2:20))
qplot(x=c(1:10), y=c(2:20))
qplot(x=c(1:10), y=c(11:20))
qplot(x=c(1:10), y=c(11:20)) + sc
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
2+2
ls()
summary(test_sum$MP)
?sin
?wav
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
head(counts_df)
print.listof(counts_df[1:50,], locale = locale(encoding = "UTF-8"))
file.edit('.Rprofile')
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
help(package = "knitr", help_type = "html")
knitr::spin("Scripts/Stoplist_Analysis_Full.R", format = "Rhtml")
knitr::spin("Scripts/Stoplist_Analysis_Full.R", format = "Rhtml")
knitr::spin("Scripts/Stoplist_Analysis_Full.R", format = "Rhtml", report = TRUE
)
knitr::spin(hair = "Scripts/Stoplist_Analysis_Full.R", format = "Rhtml", report = TRUE)
knitr::spin(hair = "Scripts/Stoplist_Analysis_Full.R", format = "Rtex", report = TRUE)
library(tinytex)
install.packages("tinytex")
library(tinytex)
knitr::spin(hair = "Scripts/Stoplist_Analysis_Full.R", format = "Rtex", report = TRUE)
knitr::spin(hair = "Scripts/Stoplist_Analysis_Full.R", format = "Rhtml", report = TRUE)
knitr::spin(hair = "Scripts/Stoplist_Analysis_Full.R", format = "Rhtml", report = TRUE)
file.edit('.Rprofile')
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
head(counts_df)
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
sessionInfo()
markdown::render("Scripts/Chinese_Trials.R")
rmarkdown::render("Scripts/Chinese_Trials.R")
getwd()
rmarkdown::render("Scripts/Chinese_Trials.R")
rmarkdown::render("Scripts/Chinese_Trials.R")
rmarkdown::render("Scripts/Chinese_Trials.R")
rmarkdown::render("Scripts/Chinese_Trials.R")
#A <- readLines(con <- file("test_eng_utf.txt", encoding = "UTF-8"))
#C <- readLines(con <- file("test_ch_utf.txt", encoding = "UTF-8"))
#C1 <- readLines(con <- file("ch1.txt", encoding = "UTF-8"))
T <- readLines(con <- file("ogt_utf.txt", encoding = "UTF-8"))
#Sys.setlocale(locale = "Chinese") #Choose Chinese
#Sys.setlocale() #Revert back to original when finished
single_counts <- table(unlist(strsplit(T, split="")))
single_counts <- single_counts[order(single_counts, decreasing = TRUE)]
single_counts[1:100]
saveRDS(single_counts, "single_counts.Rda")
x <- "xxyyxyxy"
sst <- strsplit(x, "")[[1]]
paste0(sst[c(TRUE, FALSE)], sst[c(FALSE, TRUE)])
x <- "xxyyxyxy"
sst <- unlist(strsplit(x, ""))
sst <- sst[2:length(sst)]
paste0(sst[c(TRUE, FALSE)], sst[c(FALSE, TRUE)])
ch2split <- function(x){
if (length(x)>1){
stop("Vectors are not allowed")
}
if (nchar(x)!=0){
x1 <- unlist(strsplit(x, split=""))
x2 <- x1[-1] #Need to take empty vectors into account
w1 <- paste0(x1[c(TRUE, FALSE)], x1[c(FALSE, TRUE)])
w2 <- paste0(x2[c(TRUE, FALSE)], x2[c(FALSE, TRUE)])
if(length(x1)%%2==1){
w1 <- w1[1:length(w1)-1]
}
if(length(x1)%%2==0){
w2 <- w2[1:length(w2)-1]
}
#unwanted <- “，！…”
unwanted <- "\\p{P}"
w1 <- w1[!grepl(unwanted, w1, perl=TRUE)]
w2 <- w2[!grepl(unwanted, w2, perl=TRUE)]
return(c(w1,w2))
} else {
return("")
}
}
words <- sapply(T, ch2split)
names(words) <- NULL
words <- unlist(words)
words_counts <- table(words)
words_counts <- words_counts[order(words_counts, decreasing = TRUE)]
words_counts <- as.data.frame(words_counts)
#Remove unwanted entries
words_counts <- words_counts[!(words_counts$words==""),] #Empty strings
words_counts <- words_counts[!grepl("[[:alnum:]]", words_counts$words),] #Numbers and letters
saveRDS(words_counts, "words_counts.Rda")
#library(ggplot2)
qplot(y=words_counts$Freq, x=1:length(words_counts$Freq))
library(ggplot2)
#Initial approaches
ggplot(words_counts, aes(x = words_counts$Freq)) + geom_freqpoly()
ggplot(words_counts, aes(x = log(words_counts$Freq))) + geom_freqpoly() #Rather dodgy approach with log (should be log counts, not log frequency)
#Best so far
ggplot(words_counts, aes(Freq)) +
geom_histogram(binwidth = 50) + scale_y_log10()
weliminate <- function(elim, x){
#To include some statements to prevent the possibility of input (x) being a matrix/df
y <- replicate(length(x), TRUE)
for (e in seq_along(elim)){
y <- y & !grepl(elim[e], x)
}
return(y)
}
top100 <- as.data.frame(single_counts[1:100], stringsAsFactors = FALSE)
colnames(top100) <- c("words", "Freq")
top100 <- top100$words
words_counts_no100 <- words_counts[weliminate(top100, words_counts$words),]
#T[grep("目光", T)][1:50]
getwd()
View(divideCharVector)
getwd()
#A <- readLines(con <- file("test_eng_utf.txt", encoding = "UTF-8"))
#C <- readLines(con <- file("test_ch_utf.txt", encoding = "UTF-8"))
#C1 <- readLines(con <- file("ch1.txt", encoding = "UTF-8"))
T <- readLines(con <- file("ogt_utf.txt", encoding = "UTF-8"))
source("C:/Users/Ania/Documents/Medycyna/Clinical School/Word count/Scripts/Helpful_Functions.R")
#Sys.setlocale(locale = "Chinese") #Choose Chinese
#Sys.setlocale() #Revert back to original when finished
single_counts <- table(unlist(strsplit(T, split="")))
single_counts <- single_counts[order(single_counts, decreasing = TRUE)]
single_counts[1:100]
saveRDS(single_counts, "single_counts.Rda")
x <- "xxyyxyxy"
sst <- strsplit(x, "")[[1]]
paste0(sst[c(TRUE, FALSE)], sst[c(FALSE, TRUE)])
x <- "xxyyxyxy"
sst <- unlist(strsplit(x, ""))
sst <- sst[2:length(sst)]
paste0(sst[c(TRUE, FALSE)], sst[c(FALSE, TRUE)])
ch2split <- function(x){
if (length(x)>1){
stop("Vectors are not allowed")
}
if (nchar(x)!=0){
x1 <- unlist(strsplit(x, split=""))
x2 <- x1[-1] #Need to take empty vectors into account
w1 <- paste0(x1[c(TRUE, FALSE)], x1[c(FALSE, TRUE)])
w2 <- paste0(x2[c(TRUE, FALSE)], x2[c(FALSE, TRUE)])
if(length(x1)%%2==1){
w1 <- w1[1:length(w1)-1]
}
if(length(x1)%%2==0){
w2 <- w2[1:length(w2)-1]
}
#unwanted <- “，！…”
unwanted <- "\\p{P}"
w1 <- w1[!grepl(unwanted, w1, perl=TRUE)]
w2 <- w2[!grepl(unwanted, w2, perl=TRUE)]
return(c(w1,w2))
} else {
return("")
}
}
words <- sapply(T, ch2split)
names(words) <- NULL
words <- unlist(words)
words_counts <- table(words)
words_counts <- words_counts[order(words_counts, decreasing = TRUE)]
words_counts <- as.data.frame(words_counts)
#Remove unwanted entries
words_counts <- words_counts[!(words_counts$words==""),] #Empty strings
words_counts <- words_counts[!grepl("[[:alnum:]]", words_counts$words),] #Numbers and letters
saveRDS(words_counts, "words_counts.Rda")
#library(ggplot2)
qplot(y=words_counts$Freq, x=1:length(words_counts$Freq))
library(ggplot2)
#Initial approaches
ggplot(words_counts, aes(x = words_counts$Freq)) + geom_freqpoly()
ggplot(words_counts, aes(x = log(words_counts$Freq))) + geom_freqpoly() #Rather dodgy approach with log (should be log counts, not log frequency)
#Best so far
ggplot(words_counts, aes(Freq)) +
geom_histogram(binwidth = 50) + scale_y_log10()
weliminate <- function(elim, x){
#To include some statements to prevent the possibility of input (x) being a matrix/df
y <- replicate(length(x), TRUE)
for (e in seq_along(elim)){
y <- y & !grepl(elim[e], x)
}
return(y)
}
top100 <- as.data.frame(single_counts[1:100], stringsAsFactors = FALSE)
colnames(top100) <- c("words", "Freq")
top100 <- top100$words
words_counts_no100 <- words_counts[weliminate(top100, words_counts$words),]
#T[grep("目光", T)][1:50]
getwd()
knitr::spin("Scripts/Chinese_Trials.R")
install.packages("Shiny")
install.packages("shiny")
library(shiny)
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
rmarkdown::render("Scripts/Chinese_Trials.R")
knitr::spin()
knitr::spin("Scripts/Chinese_Trials3.R")
knitr::spin("Scripts/Chinese_Trials3.R")
knitr::spin("Scripts/Chinese_Trials3.R")
knitr::spin("Scripts/Chinese_Trials3.R")
knitr::spin("Scripts/Chinese_Trials.R")
knitr::spin("Scripts/Chinese_Trials.R")
library(tm)
getwd()
cname <- "C:/Users/Ania/Documents/Medycyna/Clinical School/Word count/Other/Using_tm/Trump_Speeches/"
dir(cname)
docs <- VCorpus(DirSource(cname))
summary(docs)
inspect(docs[1])
inspect(docs[2])
docs[1]
print(docs[1])
writeLines(as.character(docs[1]))
cname <- "C:/Users/Ania/Documents/Medycyna/Clinical School/Word count/Other/Using_tm/Trump_Speeches_UTF-8//"
cname <- "C:/Users/Ania/Documents/Medycyna/Clinical School/Word count/Other/Using_tm/Trump_Speeches_UTF-8/"
docs <- VCorpus(DirSource(cname))
summary(docs)
writeLines(as.character(docs[1]))
Sys.setlocale()
writeLines(as.character(docs[1]))
file.edit('.Rprofile')
Sys.setlocale()
docs <- VCorpus(DirSource(cname))
writeLines(as.character(docs[1]))
sessionInfo()
files <- DirSource(directory = cname, encoding = "UTF-8")
docs <- VCorpus(files)
writeLines(as.character(docs[1]))
files <- DirSource(directory = cname, encoding = "UTF-8")
docs <- VCorpus(files)
writeLines(as.character(docs[1]))
writeLines(as.character(docs[8]))
docs <- tm_map(docs, removePunctuation)
inspect(docs[2])
inspect(docs[1])
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeNumbers)
inspect(docs[1])
writeLines(as.character(docs[1]))
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
for (j in seq(docs))
{
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, PlainTextDocument)
docs[1]
dtm <- DocumentTermMatrix(docs)
str(dtm)
dtm
inspect(dtm)
tdm <- TermDocumentMatrix(docs)
tdm
freq <- colSums(as.matrix(dtm))
tdm[1]
View(tdm)
View(as.data.frame(tdm))
head(tdm)
ord <- order(freq)
m <- as.matrix(dtm)
head(dtm)
head(m)
View(m)
dtms <- removeSparseTerms(dtm, 0.2)
dtms
dtm
freq <- colSums(as.matrix(dtm))
?colSums
head(table(freq), 20)
head(freq)
str(freq)
freq <- colSums(as.matrix(dtms))
head(freq)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq)
str(dtm)
str(freq)
wf <- data.frame(word=names(freq), freq=freq)
ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>50), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>1000), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>100), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>100), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=90, hjust=1))
ggplot(subset(counts_df, freq>80), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=90, hjust=1))
ggplot(subset(counts_df, freq>80), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>60), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>120), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
ggsave(filename = "counts_df_above140_freq.png", height = 8, width = 16)
getwd()
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 2)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = .2)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 0.2)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 1/5)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 1)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 1/20)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 1/400)
ggplot(subset(counts_df, freq>140), aes(x = reorder(words, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_fixed(ratio = 1/600)
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
knitr::spin("Scripts/Stoplist_Analysis_Full.R")
wordcloud(names(freq), freq, min.freq=25)
findAssocs(dtm, c("country" , "american"), corlimit=0.85) # specifying a correlation limit of 0.85
library(wordcloud)
wordcloud(names(freq), freq, min.freq=25)
wordcloud(names(freq), freq, max.words=100)
dev.off
dev.off()
wordcloud(names(freq), freq, max.words=100)
dtmss <- removeSparseTerms(dtm, 0.15)
dim(dtmss)
dtmss
View(as.matrix(dtmss))
docs_cl <- <- VCorpus(files)
docs_cl <- VCorpus(files)
inspect(docs_cl)
docs_cl[[1]]
?sprintf
c.dir <- sprintf ("%s/%s", path, contract)
files
c.dir <- sprintf ("%s/%s", "C:/Users/Ania/Documents/Medycyna/Clinical School/Word count/Other/Using_tm/Trump_Speeches_UTF-8/")
c.dir <- sprintf ("%s/%s", "C:/Users/Ania/Documents/Medycyna/Clinical School/Word count/Other/Using_tm/Trump_Speeches_UTF-8/", "sth")
c.dir
?sample
temp <- counts_df %>% filter(freq)
summary(counts_df$freq)
sum(counts_df$freq >17)
temp <- counts_df %>% filter(freq >17)
length(unique(temp$words))
View(temp)
sum(counts_df$freq >25)
sum(counts_df$freq >45)
temp <- counts_df %>% filter(freq >45)
length(unique(temp$words))
x <- "的一了他她是个不这"
x <- unlist(strsplit(x, split = ""))
x
sum(weliminate(x, temp$words))
View(temp[weliminate(x, temp$words),])
temp <- (temp[weliminate(x, temp$words),])
View(temp)
temp2 <- as.data.frame(with(temp, model.matrix(~words + 9)))
temp2 <- as.data.frame(with(temp, model.matrix(~words + 0)))
head(temp2)
colnames(temp2)
colnames(temp2) <- gsub("words", "", colnames(temp2))
colnames(temp2)
temp <- cbind(temp[, c("doc")], temp2)
dim(temp)
View(temp)
temp <- counts_df %>% filter(freq >65)
x <- "的一了他她是个不这"
x <- unlist(strsplit(x, split = ""))
temp <- (temp[weliminate(x, temp$words),])
temp2 <- as.data.frame(with(temp, model.matrix(~words + 0)))
colnames(temp2) <- gsub("words", "", colnames(temp2))
temp <- cbind(temp[, c("doc")], temp2)
temp <- as.data.frame(temp)
dim(temp)
install.packages("class")
library(class)
sample(10, ceiling(5))
set.seed(0)
train.idx <- sample(nrow(temp), ceiling(nrow(temp)*0.7))
test.idx <- (1:nrow(temp))[-train.idx]
head(train.idx)
head(test.idx)
View(train.idx)
tempu <- temp[, !colnames(temp) %in% "doc"]
templ <- temp[, "doc"]
set.seed(0)
train.idx <- sample(nrow(temp), ceiling(nrow(temp)*0.7))
test.idx <- (1:nrow(temp))[-train.idx]
knn.pred <- knn(tempu[train.idx, ], tempu[test.idx, ], templ[train.idx])
colnames(temp)
temp <- counts_df %>% filter(freq >65)
x <- "的一了他她是个不这"
x <- unlist(strsplit(x, split = ""))
temp <- (temp[weliminate(x, temp$words),])
temp2 <- as.data.frame(with(temp, model.matrix(~words + 0)))
colnames(temp2) <- gsub("words", "", colnames(temp2))
temp <- cbind(temp[, c("doc")], temp2)
colnames(temp)[1] <- "doc"
temp <- as.data.frame(temp)
tempu <- temp[, !colnames(temp) %in% "doc"]
templ <- temp[, "doc"]
set.seed(0)
train.idx <- sample(nrow(temp), ceiling(nrow(temp)*0.7))
test.idx <- (1:nrow(temp))[-train.idx]
knn.pred <- knn(tempu[train.idx, ], tempu[test.idx, ], templ[train.idx])
head(knn.pred)
str(knn.pred)
conf.mat <- table("predictions"= knn.pred, Actual = templ[test.idx])
View(conf.mat)
head(templ)
sum(conf.mat$Freq)
str(conf.mat)
sum(as.data.frame(conf.mat$Freq))
sum(as.data.frame(conf.mat)$Freq)
dim(temp)
0.7 *994
0.3 *994
str(knn.pred)
length(knn.pred)
(accuracy <- sum(diag(conf.mat)) / length(test.idx)*100)
temp3 <- cbind(knn_pred = knn.pred, templ[train.idx])
length(train.idx)
length(test.idx)
length(knn.pred)
temp3 <- cbind(knn_pred = knn.pred, templ[test.idx])
View(temp3)
temp3 <- cbind(knn_pred = knn.pred, actual =templ[test.idx])
View(temp3)
temp3 <- cbind(knn_pred = knn.pred, actual =templ[test.idx], tempu[test.idx])
temp3 <- cbind(knn_pred = knn.pred, actual =templ[test.idx], tempu[test.idx,])
dim(temp3)
head(temp3)
